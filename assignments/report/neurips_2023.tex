\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[preprint]{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\title{Six-Axis Collaborative Robot Arm Platform for Vision-Language-Action Research}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\begin{document}

\maketitle


\begin{abstract}
  This proposal presents a project to develop and implement a VLA (Vision-Language-Action) system on a custom-designed six-axis collaborative robot arm. Building upon our previous work in constructing a low-cost, high-performance robotic arm platform for embodied intelligence research, we aim to integrate modern AI algorithms that enable the robot to understand natural language instructions, recognize objects through vision systems, determine optimal grasping positions, and execute precise motion planning. The project will implement a multimodal architecture combining a Large Language Model (LLM) for instruction parsing, a Visual Language Model for zero-shot object detection, GraspNet for grasp pose prediction, and MoveIt for motion planning. We will validate the system through simulation experiments in MuJoCo, demonstrating its ability to perform pick-and-place tasks based on verbal commands. This research contributes to the field of embodied intelligence by providing an accessible platform that balances manufacturing precision and cost, making advanced robotics research more accessible to the broader research community.
\end{abstract}


\section{Introduction}

The integration of artificial intelligence with robotics has led to significant advancements in embodied intelligenceâ€”systems that combine software and hardware to interact with the physical world. Traditional industrial collaborative robot arms, designed primarily for manufacturing environments, often prioritize precision and speed at the expense of cost and accessibility, creating barriers for embodied intelligence research.

In our previous work, we designed a six-axis collaborative robot arm for embodied intelligence research, balancing precision with cost-effectiveness. Building on this foundation, this project aims to implement a comprehensive Vision-Language-Action (VLA) system. By integrating a VLA system with our accessible platform, we seek to advance embodied intelligence research while maintaining practicality and cost-effectiveness.

The significance of this project lies in its potential to democratize access to advanced robotics research capabilities, lowering the barrier to entry for researchers and accelerating progress in this rapidly evolving field.

\section{Related Work}

The integration of vision, language, and action capabilities has become central to robotics research. Key components in modern VLA systems include: (1) Large Language Models like Google's PaLM and OpenAI's GPT series for language understanding [1]; (2) Vision Language Models like OWL for zero-shot object detection [2]; (3) Networks such as GraspNet for predicting optimal grasping positions [3]; and (4) Frameworks like MoveIt for motion planning [4].

Recent work on vision-language-action frameworks includes RT-1 and RT-2 for transferring web knowledge to robotic control [5], Inner Monologue for embodied reasoning [1], and CLIPort which combines "what" and "where" pathways for robotic manipulation [6]. Our project builds upon these advances by integrating multiple AI components into a cohesive system.

\section{Proposed Approach}

Our proposed VLA system integrates four key components:

\begin{enumerate}
    \item \textbf{Language Understanding:} We will use a large language model to parse natural language instructions into structured commands. For this project, we will utilize an open-source LLM that can run on consumer-grade hardware.
    
    \item \textbf{Visual Perception:} We will implement the OWL vision language model for zero-shot object detection, allowing the system to identify objects mentioned in user instructions without requiring extensive training data.
    
    \item \textbf{Grasp Pose Prediction:} GraspNet will be deployed to determine optimal grasping positions on detected objects, providing precise spatial coordinates for manipulation.
    
    \item \textbf{Motion Planning:} MoveIt, integrated with ROS, will handle trajectory planning for the robot arm, ensuring smooth and collision-free movements.
\end{enumerate}

The processing pipeline is sequential: user instructions are first parsed by the language model to extract object names and desired actions. These objects are then located in the scene by the vision model, with grasping points predicted by GraspNet. Finally, MoveIt plans and executes the necessary movements to complete the task.

\section{Experimental Design and Expected Results}

Using MuJoCo, we will create a virtual environment simulating a tabletop with everyday objects. The system will be tasked with following instructions such as "pick up the red block and place it in the left container." We will conduct 1000 simulation trials with randomly positioned objects to assess reliability and success rate.

Performance will be evaluated using five key metrics: (1) Instruction Understanding Accuracy, (2) Object Detection Accuracy, (3) Grasp Success Rate, (4) Task Completion Rate, and (5) Execution Time.

We anticipate demonstrating a functional VLA system capable of understanding and executing simple manipulation tasks with a success rate exceeding 85%. The integration of multiple AI components on our accessible robot arm platform is expected to provide several key contributions:

\begin{itemize}
    \item A cost-effective platform for embodied intelligence research that maintains sufficient capabilities for meaningful experimentation
    \item A comprehensive VLA implementation that combines state-of-the-art models in a practical system
    \item Open-source software components that can be adopted and extended by other researchers
\end{itemize}

This work has the potential to accelerate embodied intelligence research by providing more researchers with access to capable robotic platforms enhanced with modern AI capabilities, encouraging innovative applications beyond traditional industrial use cases.

\section{Conclusion}

This project proposes the implementation of a comprehensive Vision-Language-Action system on a custom-designed six-axis collaborative robot arm. By combining language understanding, visual perception, grasp prediction, and motion planning on an accessible platform, we aim to advance embodied intelligence research while making it more accessible to a broader community. The results will contribute to bridging the gap between AI algorithms and physical robotic systems, bringing us closer to robots that can naturally understand and interact with human environments.

\section*{References}

\begin{enumerate}
\item Huang, W., Xia, F., Xiao, T., et al. (2022). Inner Monologue: Embodied Reasoning through Planning with Language Models. \textit{arXiv:2207.05608}.

\item Singh, I., Blukis, V., Mousavian, A., et al. (2022). ProgPrompt: Generating Situated Robot Task Plans using Large Language Models. \textit{IEEE International Conference on Robotics and Automation (ICRA)}, 11523-11530.

\item Chi, C., Feng, S., Du, Y., et al. (2023). Diffusion Policy: Visuomotor Policy Learning via Action Diffusion. \textit{arXiv:2303.04137}.

\item Belkhale, S., Ding, T., Xiao, T., et al. (2024). RT-H: Action Hierarchies Using Language. \textit{arXiv:2403.01823}.

\item Brohan, A., et al. (2023). RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. \textit{arXiv:2307.15818}.

\item Shridhar, M., Manuelli, L., Fox, D. (2021). CLIPort: What and Where Pathways for Robotic Manipulation. \textit{arXiv:2109.12098}.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}